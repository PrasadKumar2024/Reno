import os
import uuid
import logging
import json
import asyncio
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any
from datetime import datetime
import fitz  
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from app.models import Document, Client, KnowledgeChunk
from app.services.gemini_service import gemini_service
from app.services.pinecone_service import pinecone_service

logger = logging.getLogger(__name__)

class DocumentService:
    """
    Comprehensive document processing service with Pinecone integration
    Handles PDF processing, chunking, vector storage, and semantic search
    """
    def __init__(self):
        """Initialize document service with dependencies"""
        self.gemini_service = gemini_service
        self.pinecone_service = pinecone_service
        # CHANGE 3: Reduced chunk size from 1000 -> 512 for better precision
        # This helps vector search find specific answers in smaller text blocks.
        self.max_chunk_size = 512 
        self.chunk_overlap = 100  # Reduced overlap slightly to match        
        self.max_retries = 3
            
    
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def extract_text_from_pdf(self, file_path: str) -> str:
        logger.info("‚úÖ‚úÖ‚úÖ New PyMuPDF Extractor is now active! ‚úÖ‚úÖ‚úÖ")
        """
        Robust text extraction using PyMuPDF (fitz).
        Includes validation, resource management, and scanned document detection.
        """
        if not os.path.exists(file_path):
            logger.error(f"‚ùå File not found: {file_path}")
            raise FileNotFoundError(f"PDF file does not exist: {file_path}")

        full_text = []
        
        try:
            logger.info(f"üìÑ Opening PDF with PyMuPDF: {file_path}")
            
            # Use context manager to ensure file is properly closed
            with fitz.open(file_path) as doc:
                total_pages = len(doc)
                logger.info(f"üìÑ Processing {total_pages} pages...")

                for page_num, page in enumerate(doc, 1):
                    # Extract text preserving blocks/layout order
                    page_text = page.get_text("text")
                    
                    if page_text.strip():
                        # Add page marker to help the AI understand structure
                        cleaned_page_text = page_text.strip()
                        full_text.append(f"--- Page {page_num} ---\n{cleaned_page_text}")
                    else:
                        logger.warning(f"‚ö†Ô∏è Page {page_num} is empty or contains only images.")

            # Combine all text
            final_text = "\n\n".join(full_text)
            
            # Final Quality Check
            if not final_text.strip():
                error_msg = "‚ùå Extraction failed: Document appears to be empty or scanned images only (no selectable text)."
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            if len(final_text) < 50:
                logger.warning(f"‚ö†Ô∏è Warning: Extracted text is dangerously short ({len(final_text)} chars). Check PDF content.")

            # Clean up excessive invisible characters but keep newlines for structure
            # This removes weird PDF artifacts like multiple tabs or non-breaking spaces
            final_text_cleaned = " ".join(final_text.split())
            
            logger.info(f"‚úÖ Successfully extracted {len(final_text_cleaned)} characters from {total_pages} pages.")
            return final_text_cleaned

        except fitz.FileDataError:
            logger.error("‚ùå The file is corrupted or is not a valid PDF.")
            raise
        except Exception as e:
            logger.error(f"‚ùå Unexpected error during PDF extraction: {str(e)}")
            raise            
    
    def chunk_text(self, text: str, chunk_size: int = None, overlap: int = None) -> List[Dict[str, Any]]:
        """
        ULTRA-SIMPLE chunking - NO HELPER METHODS, NO CRASHES
        Works for ANY document, requires ZERO additional code
        """
        try:
            import re
        
            # Step 1: Clean text
            text = text.replace('\r\n', '\n').replace('\r', '\n')
            text = re.sub(r'\n\n\n+', '\n\n', text)
            text = text.strip()
        
            if not text or len(text) < 20:
                logger.warning("‚ö†Ô∏è Text too short to chunk")
                return []
        
            logger.info(f"üìÑ Processing {len(text)} chars")
        
            # Step 2: Split by paragraphs (double newline)
            paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 20]
        
            # Step 3: If no clear paragraphs, split by single newlines
            if len(paragraphs) <= 1:
                logger.info("‚ö†Ô∏è No paragraphs found, splitting by lines")
                paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 20]
        
        # Step 4: If STILL no good splits, just chunk by size
            if len(paragraphs) <= 1 and len(text) > 500:
                logger.info("‚ö†Ô∏è Using fixed-size chunking")
                paragraphs = []
                for i in range(0, len(text), 300):
                    chunk = text[i:i+300].strip()
                    if len(chunk) > 50:
                        paragraphs.append(chunk)
        
        # Step 5: Create chunk objects
            chunks = []
            for i, para in enumerate(paragraphs):
                chunks.append({
                    "chunk_text": para,
                    "start_pos": 0,
                    "end_pos": len(para),
                    "char_count": len(para),
                    "word_count": len(para.split()),
                    "chunk_index": i
                })
        
        # Step 6: Fallback if everything failed
            if not chunks:
                logger.warning("‚ö†Ô∏è No chunks created, returning whole text")
                chunks = [{
                    "chunk_text": text,
                    "start_pos": 0,
                    "end_pos": len(text),
                    "char_count": len(text),
                    "word_count": len(text.split()),
                    "chunk_index": 0
                }]
        
            logger.info(f"‚úÖ Created {len(chunks)} chunks")
        
        # Step 7: Log first 3 chunks for debugging
            for i, chunk in enumerate(chunks[:3]):
                preview = chunk['chunk_text'][:50].replace('\n', ' ')
                logger.info(f"  Chunk {i+1}: {chunk['char_count']} chars | '{preview}...'")
        
            return chunks
        
        except Exception as e:
            logger.error(f"‚ùå Chunking error: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        # Emergency fallback
            return [{
                "chunk_text": text,
                "start_pos": 0,
                "end_pos": len(text),
                "char_count": len(text),
                "word_count": len(text.split()),
                "chunk_index": 0
            }]        
    def _find_optimal_breakpoint(self, text: str, start: int, end: int) -> int:
        """
        Find optimal break point for chunking at natural boundaries
        
        Args:
            text: Full text
            start: Start position
            end: End position
            
        Returns:
            Optimal break position
        """
        # Priority order of breakpoints
        breakpoints = [
            ('\n\n', 2),      # Paragraph break
            ('. ', 2),        # Sentence end with space
            ('.\n', 2),       # Sentence end with newline
            ('? ', 2),        # Question end
            ('! ', 2),        # Exclamation end
            ('\n', 1),        # Line break
            ('.', 1),         # Sentence end without space
            ('?', 1),         # Question end
            ('!', 1),         # Exclamation end
            (',', 1),         # Comma
            (';', 1),         # Semicolon
            (' ', 1),         # Space
        ]
        
        # Search from end backwards
        search_start = max(start, end - 100)  # Look in last 100 chars
        
        for delimiter, min_length in breakpoints:
            break_pos = text.rfind(delimiter, search_start, end)
            if break_pos > start and (break_pos + len(delimiter)) - start >= min_length:
                return break_pos + len(delimiter)
        
        return end  # No good break found, use original end
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def process_document_async(self, document_id: str, db: Session) -> Dict[str, Any]:
        """
        Async document processing with Pinecone integration
        
        Args:
            document_id: UUID of the document to process
            db: Database session
            
        Returns:
            Processing results with statistics
        """
        try:
            # Get document from database
            document = db.query(Document).filter(Document.id == document_id).first()
            if not document:
                logger.error(f"Document {document_id} not found")
                return {"success": False, "error": "Document not found"}
            
            logger.info(f"üîÑ Processing document: {document.filename} (ID: {document_id})")
            
            # Check if file exists
            if not os.path.exists(document.file_path):
                logger.error(f"File not found: {document.file_path}")
                document.processed = False
                document.processing_error = "File not found"
                db.commit()
                return {"success": False, "error": "File not found"}
            
            # Extract text from PDF
            try:
                text = self.extract_text_from_pdf(document.file_path)
            except Exception as e:
                logger.error(f"Failed to extract text: {e}")
                document.processed = False
                document.processing_error = str(e)
                db.commit()
                return {"success": False, "error": f"Text extraction failed: {e}"}
            
            if not text or len(text.strip()) < 50:
                logger.warning(f"No meaningful text extracted: {len(text or '')} chars")
                document.processed = True
                document.processed_at = datetime.utcnow()
                document.processing_error = "No text content found"
                db.commit()
                return {"success": True, "warning": "No meaningful text found", "chunks_created": 0}
            
            # Split into chunks
            chunks_data = self.chunk_text(text)
            
            if not chunks_data:
                logger.warning("No chunks created from document")
                document.processed = True
                document.processed_at = datetime.utcnow()
                document.processing_error = "Failed to create text chunks"
                db.commit()
                return {"success": True, "warning": "No chunks created", "chunks_created": 0}
            
            # Delete existing chunks for this document
            deleted_count = db.query(KnowledgeChunk).filter(
                KnowledgeChunk.document_id == document_id
            ).delete()
            
            if deleted_count > 0:
                logger.info(f"üóëÔ∏è Deleted {deleted_count} existing chunks")
            
            # Delete existing vectors from Pinecone
            try:
                await self.pinecone_service.delete_document_vectors(str(document_id))
                logger.info(f"üóëÔ∏è Deleted existing Pinecone vectors for document {document_id}")
            except Exception as e:
                logger.warning(f"Could not delete Pinecone vectors: {e}")
            
            # Prepare chunks for database and Pinecone
            db_chunks = []
            pinecone_chunks = []
            
            for i, chunk_data in enumerate(chunks_data):
                chunk_id = str(uuid.uuid4())
                
                # Database chunk
                db_chunk = KnowledgeChunk(
                    id=chunk_id,
                    client_id=document.client_id,
                    document_id=document.id,
                    chunk_text=chunk_data["chunk_text"],
                    chunk_index=i,
                    chunk_metadata=json.dumps({
                        "filename": document.filename,
                        "total_chunks": len(chunks_data),
                        "char_count": chunk_data["char_count"],
                        "word_count": chunk_data["word_count"],
                        "chunk_number": i + 1,
                        "start_pos": chunk_data["start_pos"],
                        "end_pos": chunk_data["end_pos"]
                    })
                )
                db_chunks.append(db_chunk)
                
                # Pinecone chunk data
                pinecone_chunk = {
                    "chunk_text": chunk_data["chunk_text"],
                    "metadata": {
                        "chunk_id": chunk_id,
                        "client_id": str(document.client_id),
                        "document_id": str(document.id),
                        "chunk_index": i,
                        "filename": document.filename,
                        "source": "document_upload",
                        "timestamp": datetime.utcnow().isoformat()
                    }
                }
                pinecone_chunks.append(pinecone_chunk)
            
            # Store chunks in database
            for chunk in db_chunks:
                db.add(chunk)
            
            # Store chunks in Pinecone
            pinecone_stored = 0
            if self.pinecone_service.is_configured():
                try:
                    pinecone_stored = await self.pinecone_service.store_knowledge_chunks(
                        client_id=str(document.client_id),
                        chunks=pinecone_chunks
                    )
                    logger.info(f"‚úÖ Stored {pinecone_stored}/{len(pinecone_chunks)} chunks in Pinecone")
                except Exception as e:
                    logger.error(f"‚ùå Failed to store chunks in Pinecone: {e}")
                    # Continue even if Pinecone fails - database chunks are primary
            
            # Update document status
            document.processed = True
            document.processed_at = datetime.utcnow()
            document.processing_error = None
            db.commit()
            
            logger.info(f"‚úÖ Successfully processed {document.filename}: {len(db_chunks)} DB chunks, {pinecone_stored} Pinecone vectors")
            
            return {
                "success": True,
                "document_id": str(document_id),
                "chunks_created": len(db_chunks),
                "pinecone_stored": pinecone_stored,
                "total_text_length": len(text),
                "filename": document.filename
            }
            
        except Exception as e:
            
            logger.error(f"‚ùå Error processing document {document_id}: {e}")
            
            # Update document with error
            try:
                document = db.query(Document).filter(Document.id == document_id).first()
                if document:
                    document.processed = False
                    document.processing_error = str(e)
                    db.commit()
            except Exception as update_error:
                logger.error(f"Failed to update document error status: {update_error}")
            
            return {"success": False, "error": str(e)}
    
    def process_document_sync(self, document_id: str, db: Session) -> bool:
        """
        Synchronous wrapper for document processing
        
        Args:
            document_id: UUID of the document to process
            db: Database session
            
        Returns:
            True if processing succeeded
        """
        try:
            # Run async processing in event loop
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(self.process_document_async(document_id, db))
            loop.close()
            
            return result.get("success", False)
            
        except Exception as e:
            logger.error(f"Error in sync document processing: {e}")
            return False
    
    async def get_relevant_context(self, client_id: str, query: str, db: Session, max_chunks: int = 5) -> str:
        """
        Retrieve relevant context using Pinecone semantic search with database fallback
        
        Args:
            client_id: UUID of the client
            query: Search query
            db: Database session
            max_chunks: Maximum number of chunks to return
            
        Returns:
            Concatenated text from relevant chunks
        """
        try:
            # Try Pinecone semantic search first
            if self.pinecone_service.is_configured():
                try:
                    similar_chunks = await self.pinecone_service.search_similar_chunks(
                        client_id=str(client_id),
                        query=query,
                        top_k=max_chunks,
                        min_score=0.35
                    )
                    
                    if similar_chunks:
                        context_text = "\n\n".join([chunk["chunk_text"] for chunk in similar_chunks])
                        logger.info(f"‚úÖ Found {len(similar_chunks)} relevant chunks via Pinecone")
                        return context_text
                    else:
                        logger.info("No relevant chunks found via Pinecone, falling back to database")
                        
                except Exception as e:
                    logger.warning(f"Pinecone search failed, falling back to database: {e}")
            
            # Fallback to database keyword search
            return self._get_relevant_context_from_db(client_id, query, db, max_chunks)
            
        except Exception as e:
            logger.error(f"Error retrieving context for client {client_id}: {e}")
            return self._get_relevant_context_from_db(client_id, query, db, max_chunks)
    
    def _get_relevant_context_from_db(self, client_id: str, query: str, db: Session, max_chunks: int = 5) -> str:
        """
        Fallback context retrieval using database keyword matching
        
        Args:
            client_id: UUID of the client
            query: Search query
            db: Database session
            max_chunks: Maximum number of chunks to return
            
        Returns:
            Concatenated text from relevant chunks
        """
        try:
            chunks = db.query(KnowledgeChunk).filter(
                KnowledgeChunk.client_id == client_id
            ).all()
            
            if not chunks:
                logger.warning(f"No knowledge chunks found for client {client_id}")
                return ""
            
            # Simple keyword matching
            query_lower = query.lower()
            query_words = [word for word in query_lower.split() if len(word) > 3]
            
            if not query_words:
                return "\n\n".join([chunk.chunk_text for chunk in chunks[:max_chunks]])
            
            scored_chunks = []
            for chunk in chunks:
                score = 0
                chunk_lower = chunk.chunk_text.lower()
                
                for word in query_words:
                    occurrences = chunk_lower.count(word)
                    score += occurrences * 2
                    
                    if query_lower in chunk_lower:
                        score += 10
                
                if score > 0:
                    scored_chunks.append((score, chunk))
            
            if not scored_chunks:
                return "\n\n".join([chunk.chunk_text for chunk in chunks[:max_chunks]])
            
            scored_chunks.sort(reverse=True, key=lambda x: x[0])
            top_chunks = [chunk.chunk_text for score, chunk in scored_chunks[:max_chunks]]
            
            return "\n\n".join(top_chunks)
            
        except Exception as e:
            logger.error(f"Error in database context retrieval: {e}")
            return ""
    
    async def reprocess_all_documents(self, client_id: str, db: Session) -> Dict[str, Any]:
        """
        Reprocess all documents for a client with Pinecone integration
        
        Args:
            client_id: UUID of the client
            db: Database session
            
        Returns:
            Reprocessing statistics
        """
        try:
            logger.info(f"üîÑ Reprocessing all documents for client {client_id}")
            
            # Delete all existing knowledge chunks for this client
            deleted_count = db.query(KnowledgeChunk).filter(
                KnowledgeChunk.client_id == client_id
            ).delete()
            logger.info(f"üóëÔ∏è Deleted {deleted_count} existing DB chunks")
            
            # Delete all vectors for this client from Pinecone
            if self.pinecone_service.is_configured():
                try:
                    await self.pinecone_service.delete_client_vectors(str(client_id))
                    logger.info(f"üóëÔ∏è Deleted all Pinecone vectors for client {client_id}")
                except Exception as e:
                    logger.warning(f"Could not delete Pinecone vectors: {e}")
            
            # Get all documents for this client
            documents = db.query(Document).filter(
                Document.client_id == client_id
            ).all()
            
            if not documents:
                logger.warning(f"No documents found for client {client_id}")
                return {
                    "success": True,
                    "documents_processed": 0,
                    "total_documents": 0,
                    "message": "No documents found"
                }
            
            processed_count = 0
            total_chunks = 0
            failed_documents = []
            
            # Process each document
            for document in documents:
                logger.info(f"Processing document: {document.filename}")
                result = await self.process_document_async(str(document.id), db)
                
                if result.get("success"):
                    processed_count += 1
                    total_chunks += result.get("chunks_created", 0)
                else:
                    failed_documents.append({
                        "filename": document.filename,
                        "error": result.get("error", "Unknown error")
                    })
            
            logger.info(f"‚úÖ Reprocessed {processed_count}/{len(documents)} documents, created {total_chunks} chunks")
            
            return {
                "success": True,
                "documents_processed": processed_count,
                "total_documents": len(documents),
                "total_chunks_created": total_chunks,
                "failed_documents": failed_documents
            }
            
        except Exception as e:
            db.rollback()
            logger.error(f"‚ùå Error reprocessing documents for client {client_id}: {e}")
            return {
                "success": False,
                "error": str(e),
                "documents_processed": 0,
                "total_documents": 0
            }
    
    async def sync_chunks_to_pinecone(self, client_id: str, db: Session) -> Dict[str, Any]:
        """
        Sync existing database chunks to Pinecone
        
        Args:
            client_id: UUID of the client
            db: Database session
            
        Returns:
            Sync statistics
        """
        try:
            if not self.pinecone_service.is_configured():
                return {"success": False, "error": "Pinecone not configured"}
            
            # Get all chunks for this client
            chunks = db.query(KnowledgeChunk).filter(
                KnowledgeChunk.client_id == client_id
            ).all()
            
            if not chunks:
                return {"success": True, "message": "No chunks to sync", "chunks_synced": 0}
            
            logger.info(f"üîÑ Syncing {len(chunks)} database chunks to Pinecone for client {client_id}")
            
            # Prepare chunks for Pinecone
            pinecone_chunks = []
            for chunk in chunks:
                pinecone_chunk = {
                    "chunk_text": chunk.chunk_text,
                    "metadata": {
                        "chunk_id": str(chunk.id),
                        "client_id": str(client_id),
                        "document_id": str(chunk.document_id),
                        "chunk_index": chunk.chunk_index,
                        "source": "database_sync",
                        "timestamp": datetime.utcnow().isoformat()
                    }
                }
                pinecone_chunks.append(pinecone_chunk)
            
            # Store in Pinecone
            stored_count = await self.pinecone_service.store_knowledge_chunks(
                client_id=str(client_id),
                chunks=pinecone_chunks
            )
            
            logger.info(f"‚úÖ Synced {stored_count}/{len(chunks)} chunks to Pinecone")
            
            return {
                "success": True,
                "chunks_synced": stored_count,
                "total_chunks": len(chunks),
                "client_id": str(client_id)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error syncing chunks to Pinecone: {e}")
            return {"success": False, "error": str(e)}
    
    def get_document_stats(self, client_id: str, db: Session) -> Dict[str, Any]:
        """
        Get comprehensive document statistics including Pinecone data
        
        Args:
            client_id: UUID of the client
            db: Database session
            
        Returns:
            Dictionary with document statistics
        """
        try:
            # Database statistics
            total_documents = db.query(Document).filter(
                Document.client_id == client_id
            ).count()
            
            processed_documents = db.query(Document).filter(
                Document.client_id == client_id,
                Document.processed == True
            ).count()
            
            total_chunks = db.query(KnowledgeChunk).filter(
                KnowledgeChunk.client_id == client_id
            ).count()
            
            total_size = db.query(Document).filter(
                Document.client_id == client_id
            ).with_entities(db.func.sum(Document.file_size)).scalar() or 0
            
            # Pinecone statistics
            pinecone_stats = {}
            if self.pinecone_service.is_configured():
                try:
                    pinecone_stats = self.pinecone_service.get_index_stats()
                    pinecone_vector_count = asyncio.run(
                        self.pinecone_service.get_client_vector_count(str(client_id))
                    )
                    pinecone_stats["client_vectors"] = pinecone_vector_count
                except Exception as e:
                    logger.warning(f"Could not get Pinecone stats: {e}")
                    pinecone_stats = {"error": str(e)}
            
            return {
                "database": {
                    "total_documents": total_documents,
                    "processed_documents": processed_documents,
                    "pending_documents": total_documents - processed_documents,
                    "total_chunks": total_chunks,
                    "total_size_bytes": total_size,
                    "total_size_mb": round(total_size / (1024 * 1024), 2)
                },
                "pinecone": pinecone_stats,
                "services": {
                    "pinecone_configured": self.pinecone_service.is_configured(),
                    "gemini_configured": self.gemini_service.check_availability()
                }
            }
            
        except Exception as e:
            logger.error(f"Error getting document stats for client {client_id}: {e}")
            return {
                "database": {
                    "total_documents": 0,
                    "processed_documents": 0,
                    "pending_documents": 0,
                    "total_chunks": 0,
                    "total_size_bytes": 0,
                    "total_size_mb": 0
                },
                "pinecone": {"error": str(e)},
                "services": {
                    "pinecone_configured": False,
                    "gemini_configured": False
                }
            }
    
    
    @retry( stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=2, max=5)
    )
    def validate_and_save_pdf(self, file, client_id: str, upload_dir: str, original_filename: str) -> str:
        """
        Validate and save uploaded PDF file with enhanced validation using PyMuPDF ONLY
        """
        try:
            # Validate file extension
            if not original_filename.lower().endswith('.pdf'):
                raise ValueError("Only PDF files are allowed")
            
            # Validate file size (max 50MB)
            file.file.seek(0, 2)
            file_size = file.file.tell()
            file.file.seek(0)
            
            if file_size > 50 * 1024 * 1024:
                raise ValueError("File size must be less than 50MB")
            
            if file_size == 0:
                raise ValueError("File is empty")
            
            # Generate unique filename and path
            file_extension = os.path.splitext(original_filename)[1]
            unique_filename = f"{uuid.uuid4()}{file_extension}"
            file_path = os.path.join(upload_dir, unique_filename)
            
            # Ensure upload directory exists
            os.makedirs(upload_dir, exist_ok=True)
            
            # Save file
            with open(file_path, "wb") as buffer:
                content = file.file.read()
                buffer.write(content)
            
            logger.info(f"üíæ Saved file: {file_path} ({len(content)} bytes)")
            
            # ‚úÖ VALIDATE WITH PyMuPDF ONLY
            try:
                # Use fitz (PyMuPDF) to confirm it is a valid PDF
                doc = fitz.open(file_path)
                num_pages = len(doc)
                doc.close()
                logger.info(f"‚úÖ PDF validated with PyMuPDF: {num_pages} pages")
            except Exception as e:
                # Delete invalid file
                if os.path.exists(file_path):
                    os.remove(file_path)
                raise ValueError(f"Invalid or corrupted PDF file: {str(e)}")
            
            return file_path
            
        except Exception as e:
            logger.error(f"Error saving PDF file: {e}")
            raise
                
    async def health_check(self) -> Dict[str, Any]:
        """
        Comprehensive health check for document service
        
        Returns:
            Health status dictionary
        """
        try:
            # Check dependencies
            pinecone_health = await self.pinecone_service.health_check()
            gemini_health = await self.gemini_service.health_check()
            
            health_status = {
                "service": "document_processor",
                "status": "healthy",
                "timestamp": datetime.utcnow().isoformat(),
                "dependencies": {
                    "pinecone": pinecone_health,
                    "gemini": gemini_health
                },
                "configuration": {
                    "max_chunk_size": self.max_chunk_size,
                    "chunk_overlap": self.chunk_overlap,
                    "max_retries": self.max_retries
                }
            }
            
            # Determine overall status
            if not pinecone_health.get("healthy") or not gemini_health.get("healthy"):
                health_status["status"] = "degraded"
                health_status["issues"] = "Some dependencies are unhealthy"
            
            return health_status
            
        except Exception as e:
            logger.error(f"Document service health check failed: {e}")
            return {
                "service": "document_processor",
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }


# Global singleton instance
document_service = DocumentService()
